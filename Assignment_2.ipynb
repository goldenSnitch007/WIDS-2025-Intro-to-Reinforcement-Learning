{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making MDPs in Python\n",
    "\n",
    "For the simple environments, we can just hardcode the MDPs into a dictionary by exhaustively encoding the whole state space and the transition function. We will also go through a more complicated example where the state space is too large to be manually coded and we need to implement the transition function based on some state parameters.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-2.\n",
    "\n",
    "NOTE: all page numbers referred to in this ipynb notebook refer to Grokking textbook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 0 - Bandit Walk\n",
    "\n",
    "Let us consider the BW environment on Page 39. \n",
    "\n",
    "State Space has 3 elements, states 0, 1 and 2.\n",
    "States 0 and 2 are terminal states and state 1 is the starting state.\n",
    "\n",
    "Action space has 2 elements, left and right.\n",
    "\n",
    "The environment is deterministic - transition probability of any action is 1.\n",
    "\n",
    "Only 1 (State, Action, State') tuple has positive reward, (1, Right, 2) gives the agent +1 reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll model this MDP as a dictionary. This code is an example for the upcoming exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw_mdp = {\n",
    "\n",
    "    0 : {\n",
    "        \"Right\" : [(1, 0, 0, True)],\n",
    "        \"Left\" : [(1, 0, 0, True)]\n",
    "    },\n",
    "\n",
    "    1 : {\n",
    "        \"Right\" : [(1, 2, 1, True)],\n",
    "        \"Left\" : [(1, 0, 0, True)]\n",
    "    },\n",
    "\n",
    "    2 : {\n",
    "        \"Right\" : [(1, 2, 0, True)],\n",
    "        \"Left\" : [(1, 2, 0, True)]\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we a dictionary bw_mdp which to get the transitions possible we can do bw_mdp\\[current state\\]\\[action to take\\]. this will return a list of the possibilities.\n",
    "\n",
    "The format of each element within the list is   \n",
    "(probability of that transition , next state , reward , 'is the next state a terminal state?')\n",
    "\n",
    "Obviously make sure that the sum of probability of all possible transitions is 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by convention, all actions from terminal states still lead to the same state with reward 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 1 - Slippery Walk\n",
    "\n",
    "Here, states 0 and 6 are terminal states and state 3 is the starting state.\n",
    "\n",
    "Action space has again 2 elements, left and right.\n",
    "\n",
    "The environment is now stochastic, transition probability of any action is as follows -\n",
    "If agent chooses `Right` at a non-terminal state,\n",
    "- $50\\%$ times it will go to `Right` state\n",
    "- $33\\frac{1}{3} \\%$ times it will stay in same state\n",
    "- $16\\frac{2}{3}\\%$ times it will go to `Left`state\n",
    "\n",
    "And similiarly vice versa for Left(just replace left with right and right with left in previous list)\n",
    "\n",
    "This time, 2 different (State, Action, State') tuples have positive rewards, you need to find them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll again model this MDP as a dictionary. Part of the code is written for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (1188849807.py, line 17)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m()\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "swf_mdp = {\n",
    "\n",
    "    0: {\n",
    "        \"Right\": [(1.0, 0, 0.0, True)],\n",
    "        \"Left\": [(1.0, 0, 0.0, True)]\n",
    "    },\n",
    "\n",
    "    1: {\n",
    "        \"Right\": [\n",
    "            (1/2, 2, 0.0, False),\n",
    "            (1/3, 1, 0.0, False),\n",
    "            (1/6, 0, 0.0, True)\n",
    "        ],\n",
    "        \"Left\": [\n",
    "            (1/2, 0, 0.0, True),\n",
    "            (1/3, 1, 0.0, False),\n",
    "            (1/6, 2, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "\n",
    "    #to be added \n",
    "\n",
    "    2: {\n",
    "        \"Right\": [\n",
    "            (1/2, 3, 0.0, False),\n",
    "            (1/3, 2, 0.0, False),\n",
    "            (1/6, 1, 0.0, False)\n",
    "        ],\n",
    "        \"Left\": [\n",
    "            (1/2, 1, 0.0, False),\n",
    "            (1/3, 2, 0.0, False),\n",
    "            (1/6, 3, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    3: {\n",
    "        \"Right\": [\n",
    "            (1/2, 4, 0.0, False),\n",
    "            (1/3, 3, 0.0, False),\n",
    "            (1/6, 2, 0.0, False)\n",
    "        ],\n",
    "        \"Left\": [\n",
    "            (1/2, 2, 0.0, False),\n",
    "            (1/3, 3, 0.0, False),\n",
    "            (1/6, 4, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    4: {\n",
    "        \"Right\": [\n",
    "            (1/2, 5, 0.0, False),\n",
    "            (1/3, 4, 0.0, False),\n",
    "            (1/6, 3, 0.0, False)\n",
    "        ],\n",
    "        \"Left\": [\n",
    "            (1/2, 3, 0.0, False),\n",
    "            (1/3, 4, 0.0, False),\n",
    "            (1/6, 5, 0.0, False)\n",
    "        ]\n",
    "    },\n",
    "    5: {\n",
    "        \"Right\": [\n",
    "            (1/2, 6, 1.0, True),\n",
    "            (1/3, 5, 0.0, False),\n",
    "            (1/6, 4, 0.0, False)\n",
    "        ],\n",
    "        \"Left\": [\n",
    "            (1/2, 4, 0.0, False),\n",
    "            (1/3, 5, 0.0, False),\n",
    "            (1/6, 6, 1.0, True)\n",
    "        ]\n",
    "    },\n",
    "    6: {\n",
    "        \"Right\": [(1.0, 6, 0.0, True)],\n",
    "        \"Left\": [(1.0, 6, 0.0, True)]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 2 - Frozen Lake Environment\n",
    "\n",
    "This environment is described on Page 46.\n",
    "\n",
    "The FL environment has a large state space, so it's better to generate most of the MDP via Python instead of typing stuff manually.\n",
    "\n",
    "Note that all 5 states - 5, 7, 11, 12, 15 are terminal states, so keep that in mind while constructing the MDP.\n",
    "\n",
    "There are 4 actions now - Up, Down, Left, Right.\n",
    "\n",
    "The environment is stochastic, and states at the border of lake will require separate treatment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet again we will model this MDP as a (large) dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_mdp = {\n",
    "    # to be added\n",
    "}\n",
    "import pprint \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Down': [(0.3333333333333333, 10, 0.0, False),\n",
      "          (0.3333333333333333, 5, 0.0, True),\n",
      "          (0.3333333333333333, 7, 0.0, True)],\n",
      " 'Left': [(0.3333333333333333, 5, 0.0, True),\n",
      "          (0.3333333333333333, 2, 0.0, False),\n",
      "          (0.3333333333333333, 10, 0.0, False)],\n",
      " 'Right': [(0.3333333333333333, 7, 0.0, True),\n",
      "           (0.3333333333333333, 2, 0.0, False),\n",
      "           (0.3333333333333333, 10, 0.0, False)],\n",
      " 'Up': [(0.3333333333333333, 2, 0.0, False),\n",
      "        (0.3333333333333333, 5, 0.0, True),\n",
      "        (0.3333333333333333, 7, 0.0, True)]}\n"
     ]
    }
   ],
   "source": [
    "rows, cols = 4,4\n",
    "holes = {5,7,11,12}\n",
    "goal = 15\n",
    "moves = {\n",
    "    \"Left\":  (0, -1),\n",
    "    \"Right\": (0, 1),\n",
    "    \"Up\":    (-1, 0),\n",
    "    \"Down\":  (1, 0)\n",
    "}\n",
    "action_outcomes = {\n",
    "    \"Left\": [\"Left\",\"Up\",\"Down\"],\n",
    "    \"Up\": [\"Up\",\"Left\",\"Right\"],\n",
    "    \"Down\":[\"Down\",\"Left\",\"Right\"],\n",
    "    \"Right\":[\"Right\",\"Up\",\"Down\"]\n",
    "}\n",
    "\n",
    "for state in range(0, 16):\n",
    "\n",
    "    # add transitions to other states\n",
    "    transitions = {}\n",
    "    if state in holes:\n",
    "        for action in [\"Left\", \"Right\", \"Up\", \"Down\"]:\n",
    "            transitions[action] = [(1.0,state,0.0,True)]\n",
    "        fl_mdp[state] = transitions\n",
    "        continue\n",
    "\n",
    "        \n",
    "    elif state == goal:\n",
    "        for action in [\"Left\", \"Right\", \"Up\", \"Down\"]:\n",
    "            transitions[action] = [(1.0,state,1,True)]\n",
    "        fl_mdp[state] = transitions\n",
    "        continue\n",
    "    row = state//rows\n",
    "    col = state%cols  \n",
    "\n",
    "    for action in [\"Up\", \"Down\", \"Right\", \"Left\"]:\n",
    "        outcomes = []\n",
    "        possible_drifts = action_outcomes[action]\n",
    "        prob = 1/3\n",
    "        for drift in possible_drifts:\n",
    "            dr,dc = moves[drift]\n",
    "\n",
    "            next_row = max(0,min(rows -1,row+dr))\n",
    "            next_col = max(0, min(cols - 1, col + dc))\n",
    "\n",
    "            next_state = next_row*cols + next_col\n",
    "            reward = 1.0 if next_state == goal else 0.0\n",
    "            is_terminal = (next_state in holes) or (next_state == goal)\n",
    "\n",
    "            outcomes.append((prob, next_state, reward, is_terminal))\n",
    "        transitions[action] = outcomes\n",
    "\n",
    "    fl_mdp[state] = transitions    \n",
    "pprint.pprint(fl_mdp[6])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to do some stuff manually, but make sure to automate most of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check your implementation of the FL environment by comparing it with the one in OpenAI Gym.\n",
    "OpenAI Gym basically contains implmentations of common RL MDPs so that we can directly train our agents on test environments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "P = gym.make('FrozenLake-v1').env.P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the imported MDP is also just a dictionary, we can just print it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'Down': [(0.3333333333333333, 4, 0.0, False),\n",
      "              (0.3333333333333333, 0, 0.0, False),\n",
      "              (0.3333333333333333, 1, 0.0, False)],\n",
      "     'Left': [(0.3333333333333333, 0, 0.0, False),\n",
      "              (0.3333333333333333, 0, 0.0, False),\n",
      "              (0.3333333333333333, 4, 0.0, False)],\n",
      "     'Right': [(0.3333333333333333, 1, 0.0, False),\n",
      "               (0.3333333333333333, 0, 0.0, False),\n",
      "               (0.3333333333333333, 4, 0.0, False)],\n",
      "     'Up': [(0.3333333333333333, 0, 0.0, False),\n",
      "            (0.3333333333333333, 0, 0.0, False),\n",
      "            (0.3333333333333333, 1, 0.0, False)]},\n",
      " 1: {'Down': [(0.3333333333333333, 5, 0.0, True),\n",
      "              (0.3333333333333333, 0, 0.0, False),\n",
      "              (0.3333333333333333, 2, 0.0, False)],\n",
      "     'Left': [(0.3333333333333333, 0, 0.0, False),\n",
      "              (0.3333333333333333, 1, 0.0, False),\n",
      "              (0.3333333333333333, 5, 0.0, True)],\n",
      "     'Right': [(0.3333333333333333, 2, 0.0, False),\n",
      "               (0.3333333333333333, 1, 0.0, False),\n",
      "               (0.3333333333333333, 5, 0.0, True)],\n",
      "     'Up': [(0.3333333333333333, 1, 0.0, False),\n",
      "            (0.3333333333333333, 0, 0.0, False),\n",
      "            (0.3333333333333333, 2, 0.0, False)]},\n",
      " 2: {'Down': [(0.3333333333333333, 6, 0.0, False),\n",
      "              (0.3333333333333333, 1, 0.0, False),\n",
      "              (0.3333333333333333, 3, 0.0, False)],\n",
      "     'Left': [(0.3333333333333333, 1, 0.0, False),\n",
      "              (0.3333333333333333, 2, 0.0, False),\n",
      "              (0.3333333333333333, 6, 0.0, False)],\n",
      "     'Right': [(0.3333333333333333, 3, 0.0, False),\n",
      "               (0.3333333333333333, 2, 0.0, False),\n",
      "               (0.3333333333333333, 6, 0.0, False)],\n",
      "     'Up': [(0.3333333333333333, 2, 0.0, False),\n",
      "            (0.3333333333333333, 1, 0.0, False),\n",
      "            (0.3333333333333333, 3, 0.0, False)]},\n",
      " 3: {'Down': [(0.3333333333333333, 7, 0.0, True),\n",
      "              (0.3333333333333333, 2, 0.0, False),\n",
      "              (0.3333333333333333, 3, 0.0, False)],\n",
      "     'Left': [(0.3333333333333333, 2, 0.0, False),\n",
      "              (0.3333333333333333, 3, 0.0, False),\n",
      "              (0.3333333333333333, 7, 0.0, True)],\n",
      "     'Right': [(0.3333333333333333, 3, 0.0, False),\n",
      "               (0.3333333333333333, 3, 0.0, False),\n",
      "               (0.3333333333333333, 7, 0.0, True)],\n",
      "     'Up': [(0.3333333333333333, 3, 0.0, False),\n",
      "            (0.3333333333333333, 2, 0.0, False),\n",
      "            (0.3333333333333333, 3, 0.0, False)]},\n",
      " 4: {'Down': [(0.3333333333333333, 8, 0.0, False),\n",
      "              (0.3333333333333333, 4, 0.0, False),\n",
      "              (0.3333333333333333, 5, 0.0, True)],\n",
      "     'Left': [(0.3333333333333333, 4, 0.0, False),\n",
      "              (0.3333333333333333, 0, 0.0, False),\n",
      "              (0.3333333333333333, 8, 0.0, False)],\n",
      "     'Right': [(0.3333333333333333, 5, 0.0, True),\n",
      "               (0.3333333333333333, 0, 0.0, False),\n",
      "               (0.3333333333333333, 8, 0.0, False)],\n",
      "     'Up': [(0.3333333333333333, 0, 0.0, False),\n",
      "            (0.3333333333333333, 4, 0.0, False),\n",
      "            (0.3333333333333333, 5, 0.0, True)]},\n",
      " 5: {'Down': [(1.0, 5, 0.0, True)],\n",
      "     'Left': [(1.0, 5, 0.0, True)],\n",
      "     'Right': [(1.0, 5, 0.0, True)],\n",
      "     'Up': [(1.0, 5, 0.0, True)]},\n",
      " 6: {'Down': [(0.3333333333333333, 10, 0.0, False),\n",
      "              (0.3333333333333333, 5, 0.0, True),\n",
      "              (0.3333333333333333, 7, 0.0, True)],\n",
      "     'Left': [(0.3333333333333333, 5, 0.0, True),\n",
      "              (0.3333333333333333, 2, 0.0, False),\n",
      "              (0.3333333333333333, 10, 0.0, False)],\n",
      "     'Right': [(0.3333333333333333, 7, 0.0, True),\n",
      "               (0.3333333333333333, 2, 0.0, False),\n",
      "               (0.3333333333333333, 10, 0.0, False)],\n",
      "     'Up': [(0.3333333333333333, 2, 0.0, False),\n",
      "            (0.3333333333333333, 5, 0.0, True),\n",
      "            (0.3333333333333333, 7, 0.0, True)]},\n",
      " 7: {'Down': [(1.0, 7, 0.0, True)],\n",
      "     'Left': [(1.0, 7, 0.0, True)],\n",
      "     'Right': [(1.0, 7, 0.0, True)],\n",
      "     'Up': [(1.0, 7, 0.0, True)]},\n",
      " 8: {'Down': [(0.3333333333333333, 12, 0.0, True),\n",
      "              (0.3333333333333333, 8, 0.0, False),\n",
      "              (0.3333333333333333, 9, 0.0, False)],\n",
      "     'Left': [(0.3333333333333333, 8, 0.0, False),\n",
      "              (0.3333333333333333, 4, 0.0, False),\n",
      "              (0.3333333333333333, 12, 0.0, True)],\n",
      "     'Right': [(0.3333333333333333, 9, 0.0, False),\n",
      "               (0.3333333333333333, 4, 0.0, False),\n",
      "               (0.3333333333333333, 12, 0.0, True)],\n",
      "     'Up': [(0.3333333333333333, 4, 0.0, False),\n",
      "            (0.3333333333333333, 8, 0.0, False),\n",
      "            (0.3333333333333333, 9, 0.0, False)]},\n",
      " 9: {'Down': [(0.3333333333333333, 13, 0.0, False),\n",
      "              (0.3333333333333333, 8, 0.0, False),\n",
      "              (0.3333333333333333, 10, 0.0, False)],\n",
      "     'Left': [(0.3333333333333333, 8, 0.0, False),\n",
      "              (0.3333333333333333, 5, 0.0, True),\n",
      "              (0.3333333333333333, 13, 0.0, False)],\n",
      "     'Right': [(0.3333333333333333, 10, 0.0, False),\n",
      "               (0.3333333333333333, 5, 0.0, True),\n",
      "               (0.3333333333333333, 13, 0.0, False)],\n",
      "     'Up': [(0.3333333333333333, 5, 0.0, True),\n",
      "            (0.3333333333333333, 8, 0.0, False),\n",
      "            (0.3333333333333333, 10, 0.0, False)]},\n",
      " 10: {'Down': [(0.3333333333333333, 14, 0.0, False),\n",
      "               (0.3333333333333333, 9, 0.0, False),\n",
      "               (0.3333333333333333, 11, 0.0, True)],\n",
      "      'Left': [(0.3333333333333333, 9, 0.0, False),\n",
      "               (0.3333333333333333, 6, 0.0, False),\n",
      "               (0.3333333333333333, 14, 0.0, False)],\n",
      "      'Right': [(0.3333333333333333, 11, 0.0, True),\n",
      "                (0.3333333333333333, 6, 0.0, False),\n",
      "                (0.3333333333333333, 14, 0.0, False)],\n",
      "      'Up': [(0.3333333333333333, 6, 0.0, False),\n",
      "             (0.3333333333333333, 9, 0.0, False),\n",
      "             (0.3333333333333333, 11, 0.0, True)]},\n",
      " 11: {'Down': [(1.0, 11, 0.0, True)],\n",
      "      'Left': [(1.0, 11, 0.0, True)],\n",
      "      'Right': [(1.0, 11, 0.0, True)],\n",
      "      'Up': [(1.0, 11, 0.0, True)]},\n",
      " 12: {'Down': [(1.0, 12, 0.0, True)],\n",
      "      'Left': [(1.0, 12, 0.0, True)],\n",
      "      'Right': [(1.0, 12, 0.0, True)],\n",
      "      'Up': [(1.0, 12, 0.0, True)]},\n",
      " 13: {'Down': [(0.3333333333333333, 13, 0.0, False),\n",
      "               (0.3333333333333333, 12, 0.0, True),\n",
      "               (0.3333333333333333, 14, 0.0, False)],\n",
      "      'Left': [(0.3333333333333333, 12, 0.0, True),\n",
      "               (0.3333333333333333, 9, 0.0, False),\n",
      "               (0.3333333333333333, 13, 0.0, False)],\n",
      "      'Right': [(0.3333333333333333, 14, 0.0, False),\n",
      "                (0.3333333333333333, 9, 0.0, False),\n",
      "                (0.3333333333333333, 13, 0.0, False)],\n",
      "      'Up': [(0.3333333333333333, 9, 0.0, False),\n",
      "             (0.3333333333333333, 12, 0.0, True),\n",
      "             (0.3333333333333333, 14, 0.0, False)]},\n",
      " 14: {'Down': [(0.3333333333333333, 14, 0.0, False),\n",
      "               (0.3333333333333333, 13, 0.0, False),\n",
      "               (0.3333333333333333, 15, 1.0, True)],\n",
      "      'Left': [(0.3333333333333333, 13, 0.0, False),\n",
      "               (0.3333333333333333, 10, 0.0, False),\n",
      "               (0.3333333333333333, 14, 0.0, False)],\n",
      "      'Right': [(0.3333333333333333, 15, 1.0, True),\n",
      "                (0.3333333333333333, 10, 0.0, False),\n",
      "                (0.3333333333333333, 14, 0.0, False)],\n",
      "      'Up': [(0.3333333333333333, 10, 0.0, False),\n",
      "             (0.3333333333333333, 13, 0.0, False),\n",
      "             (0.3333333333333333, 15, 1.0, True)]},\n",
      " 15: {'Down': [(1.0, 15, 1, True)],\n",
      "      'Left': [(1.0, 15, 1, True)],\n",
      "      'Right': [(1.0, 15, 1, True)],\n",
      "      'Up': [(1.0, 15, 1, True)]}}\n"
     ]
    }
   ],
   "source": [
    "# using the pretty print module\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(fl_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 3 - Key Door Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement a slightly different version of frozen lake without the stochasticity but with the added constraint of a key.\n",
    "\n",
    "So now we have a 4x4 grid in which some cells are holes(and so terminal) and we have a goal state(keep it as 15) but this time someone locked the goal state with a door :(. Without the key we CANNOT ENTER the goal state.\n",
    "\n",
    "Luckily for us, that guy was very clumsy and dropped the key in tile 3!. So now make a MDP to represent the process of us escaping this grid world.\n",
    "\n",
    "Remember that the state in a MDP isnt just the position/location we are in. it should represent our whole game situation. (For example I can either be at tile 14 with the key or at tile 14 without key and these two situations are different and therfore must be represented as **different** states!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarise\n",
    "- state state is 0\n",
    "- door/goal state is 15\n",
    "- holes at 5, 7, 11, 12\n",
    "- key at 3\n",
    "- The key is picked up automatically upon entering tile 3\n",
    "- can only enter tile 15 when key is obtained(otherwise its like a wall)\n",
    "- holes & goal are terminal\n",
    "- reward of 1 when reaching tile 15 wiht the key\n",
    "- Attempting to enter tile 15 when we DONT have the key should be treated like hitting a wall and bouncing back to the original state where we took that action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 14, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "# You can implements any helper function or other variables to help you implement it\n",
    "\n",
    "gw_map = {\n",
    "    #your mdp should go here\n",
    "}\n",
    "rows,cols = 4,4\n",
    "holes = {5, 7, 11, 12}\n",
    "goal = 15\n",
    "key_loc = 3\n",
    "\n",
    "#0 15 is the state withouot key and +16 of that is the same but means that a key is found\n",
    "total_state = 32\n",
    "offset = 16\n",
    "\n",
    "moves = {\n",
    "    \"Left\":  (0, -1),\n",
    "    \"Right\": (0, 1),\n",
    "    \"Up\":    (-1, 0),\n",
    "    \"Down\":  (1, 0)\n",
    "}\n",
    "\n",
    "for state in range(32):\n",
    "    # location\n",
    "    has_key = (state >  offset)\n",
    "    loc = state%16\n",
    "    transitions = {}\n",
    "\n",
    "    #termination check\n",
    "    is_dead = loc in holes\n",
    "    is_won = has_key and loc == goal\n",
    "    #if terminal \n",
    "    if is_dead or is_won:\n",
    "        for action in [\"Left\",\"Right\",\"Up\",\"Down\"]:\n",
    "            transitions[action] = [(1.0,state,0.0,True)]\n",
    "        gw_map[state] = transitions\n",
    "        continue\n",
    "\n",
    "    row = loc//4\n",
    "    col = loc % 4\n",
    "\n",
    "    for action in [\"Left\",\"Right\",\"Up\",\"Down\"]:\n",
    "        dr,dc = moves[action]\n",
    "\n",
    "        next_r = max(0 , min(3,row + dr))\n",
    "        next_c = max(0 , min(3 , col + dc))\n",
    "        next_loc = next_row * cols + next_col\n",
    "\n",
    "        if next_loc == goal and not has_key:\n",
    "            next_loc = loc\n",
    "        next_has_key = has_key \n",
    "        \n",
    "        # If we step on the key location, we get the key\n",
    "        if next_loc == key_loc:\n",
    "            next_has_key = True\n",
    "        \n",
    "        #if gto key +16\n",
    "\n",
    "        next_state = next_loc + (offset if next_has_key else 0)\n",
    "        reward = 1.0 if (next_loc == goal and next_has_key) else 0.0\n",
    "\n",
    "        dest_is_terminal = (next_loc in holes) or (next_loc == goal and next_has_key)\n",
    "        \n",
    "        transitions[action] = [(1.0, next_state, reward, dest_is_terminal)]\n",
    "\n",
    "    gw_map[state] = transitions\n",
    "print(gw_map[14][\"Right\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact way you represent each state and each action is up to you! But... do atleast write down in a comment or something how you represent the states and action in the MDP. So i don't have to spend time deciphering.\n",
    "\n",
    "Note: For additional challenges\n",
    "- Make this grid-world also stochastic like Frozen Lake\n",
    "- Make a function that allows you to input in the locations of the holes, the door(goal state) and the key and using that it will generate and return the MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 4 - Lights Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lights out** is a deterministic puzzle game which consists of a 5x5 grid of lights which can either be on or off.\n",
    "\n",
    "When the game begins a random set of these lights will be switched on. Pressing any light will toggle it (toggle means if the light was on then it will become off and if it was off then it will become on) and it will also toggle all of its neighbours.\n",
    "\n",
    "![image of the game](./images/LightsOutIllustration.png)\n",
    "\n",
    "Note: so any non-edge and non-corner cell action will toggle 5 lights, an edge cell action would toggle 4 lights and a corner cell action only toggles 3 lights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The goal of the game is to find a sequence of moves to switch off all the lights. Now i'm not gonna make you solve this instead we are going to represent this as a deterministic MDP, where \n",
    "- the states should represent the game grid layout \n",
    "- the actions are the cells you wish to toggle\n",
    "- Give a reward of 1 when solved \n",
    "- the ONLY terminal state is the fully solved state\n",
    "\n",
    "Since 5x5 is pretty big we will restrict ourselves to a **4x4** grid and represent it as an MDP instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0, 36, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "rows, cols = 4, 4\n",
    "num_lights = rows * cols  # 16\n",
    "total_states = 1 << num_lights  # This is 2^16 = 65536\n",
    "\n",
    "lo_map = {}\n",
    "\n",
    "# Pre-calculate the effect of every button ---\n",
    "# a dict where each button press is mapped to its consequences \n",
    "action_masks = {}\n",
    "\n",
    "for r in range(rows):\n",
    "    for c in range(cols):\n",
    "        action_id = r * cols + c\n",
    "        \n",
    "        # The bit for the button itself is always 1\n",
    "        mask = (1 << action_id)\n",
    "        \n",
    "        # Check the 4 neighbors (Up, Down, Left, Right)\n",
    "        for dr, dc in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n",
    "            nr, nc = r + dr, c + dc\n",
    "            \n",
    "            # If neighbor is inside grid, set its bit to 1 too\n",
    "            if 0 <= nr < rows and 0 <= nc < cols:\n",
    "                neighbor_id = nr * cols + nc\n",
    "                mask |= (1 << neighbor_id) # Turn on that specific bit\n",
    "        \n",
    "        action_masks[action_id] = mask\n",
    "\n",
    "\n",
    "for state in range(total_states):\n",
    "    \n",
    "    \n",
    "    # If we are already solved, the game is over\n",
    "    if state == 0:\n",
    "        lo_map[state] = {a: [(1.0, 0, 0.0, True)] for a in range(num_lights)}\n",
    "        continue\n",
    "\n",
    "    transitions = {}\n",
    "    \n",
    "    # Try pressing every single button (0 to 15)\n",
    "    for action in range(num_lights):\n",
    "        \n",
    "        mask = action_masks[action]\n",
    "        \n",
    "        # THE MATH: XOR the current state with the mask\n",
    "        # If a light matches the mask bit, it flips.\n",
    "        next_state = state ^ mask\n",
    "        \n",
    "        # Check if we just solved it (State became 0)\n",
    "        is_solved = (next_state == 0)\n",
    "        \n",
    "        reward = 1.0 if is_solved else 0.0\n",
    "        is_terminal = is_solved\n",
    "        \n",
    "        # Add to dictionary\n",
    "        transitions[action] = [(1.0, next_state, reward, is_terminal)]\n",
    "\n",
    "    lo_map[state] = transitions\n",
    "print(lo_map[3][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact way you represent each state and each action is up to you! But... do atleast write down in a comment or something how you represent the states and action in the MDP. So i don't have to spend time deciphering.\n",
    "\n",
    "Note: if you want you can also try to represent the 5x5 one but this is optional"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
